{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('post.csv', encoding= 'unicode_escape')\n",
    "comments = pd.read_csv('comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post shape:  (90115, 363)\n",
      "Comments shape:  (399029, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Post shape: ', posts.shape)\n",
    "print('Comments shape: ', comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.flair 공백 제거\n",
    "posts.isnull().sum()\n",
    "posts = posts[(posts.link_flair_text.notnull())]\n",
    "posts.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# 2. flair 한글만 남기고 제거\n",
    "posts['link_flair_text']=posts['link_flair_text'].str.replace(pat=r'[^ㄱ-ㅣ가-힣]+', repl= r'', regex=True)\n",
    "posts['link_flair_text']= posts['link_flair_text'].str.replace(pat=r'[^\\w]', repl=r'', regex=True)\n",
    "posts['link_flair_text'].str.strip()\n",
    "\n",
    "# 3. flair 리스트 정리\n",
    "flair_list = [\"문화\",\n",
    "                \"생활\",\n",
    "                \"경제\",\n",
    "                \"정치\",\n",
    "                \"자연\",\n",
    "                \"역사\",\n",
    "                \"레저와 취미\", \"레저\", \"취미\",\n",
    "                \"이민\",\n",
    "                \"건강\",\n",
    "                \"시장\",\n",
    "                \"개인\",\n",
    "                \"유머\",\n",
    "                \"범죄\",\n",
    "                \"부고\"]\n",
    "\n",
    "# 4. posts flair 정리\n",
    "posts = posts[posts['link_flair_text'].isin(flair_list)]\n",
    "posts['title'] = posts['title'].apply(lambda x: re.sub('[^A-Za-z]', ' ', x))\n",
    "posts = posts[~posts['selftext'].isin(['[removed]', '[deleted]'])]\n",
    "posts['selftext'] = posts['selftext'].replace(np.nan, '')\n",
    "# posts = posts[int(posts['num_comments']) >= 4]\n",
    "posts.rename(columns={'link_flair_text': 'flair'}, inplace=True)\n",
    "posts['flair'] = posts['flair'].apply(lambda x: \"레저와 취미\" if x==\"레저\" else x)\n",
    "posts['flair'] = posts['flair'].apply(lambda x: \"레저와 취미\" if x==\"취미\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'post' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c6a0952f693f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'font.family'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Malgun Gothic'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link_flair_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'post' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. post flair 분포 확인\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "posts['link_flair_text'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. post reset index\n",
    "posts.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. post 타이틀+본문\n",
    "for i in range(len(posts)) : \n",
    "     if posts['selftext'].isnull()[i] == False : \n",
    "        posts.loc[i,'title'] = str(posts.iloc[i]['title']) + str(posts.iloc[i]['selftext'])\n",
    "posts.drop('selftext',inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필요한 칼럼만 추출\n",
    "comments = comments.loc[:, ['link_id', 'body', 'score', 'created_utc']]\n",
    "\n",
    "# 2. [deleted], [removed] 등 제거\n",
    "comments = comments[~comments['body'].isin(['[deleted]', '[removed]',\"Thanks for posting your photograph on\", \"Your submission\", \"your submission\"])]\n",
    "\n",
    "# 3. link_id 앞 3자리 제거\n",
    "comments['link_id'] = comments['link_id'].apply(lambda x: x[3:])\n",
    "\n",
    "# 4. body에서 영문자, 숫자 빼고 제거\n",
    "comments['body'] = comments['body'].str.replace('[^A-Za-z1-9]+', ' ')\n",
    "\n",
    "# 5. body에서 null값 제거\n",
    "comments = comments.dropna(subset=['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "포스트 별 코멘트 말뭉치 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.딕셔너리 생성\n",
    "dict = {}\n",
    "\n",
    "for link_id, body in comments[['link_id', 'body']].values:\n",
    "    if link_id in dict:\n",
    "        dict[link_id].append(body)\n",
    "    else:\n",
    "        dict[link_id] = [body]\n",
    "        \n",
    "# 2. 말뭉치 생성\n",
    "comments_corpus = pd.DataFrame(dict.items())\n",
    "comments_corpus.columns = ['id', 'comments']\n",
    "comments_corpus\n",
    "\n",
    "# 3. 컴바인드 데이터 프레임 생성\n",
    "combined = pd.merge(posts, comments_corpus, on='id', how='left')\n",
    "combined = combined.dropna(subset=['comments'])\n",
    "combined.reset_index(drop=True,inplace=True)\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined 파일 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_tokenize(post_title):\n",
    "\n",
    "    post_text = post_title\n",
    "    post_text = re.sub(\"[^a-zA-Z]\",\" \", post_text)\n",
    "    post_text=post_text.lower()\n",
    "    post_text=replacer.replace(post_text)\n",
    "    sentence_list = sent_tokenize(post_text)\n",
    "    token_sentence_list = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        word_list = word_tokenize(sentence)\n",
    "        token_sentence_list.append(word_list)\n",
    "        \n",
    "    for token_sentence in token_sentence_list:\n",
    "        for token in token_sentence:\n",
    "            if token in english_stop_list:\n",
    "                token_sentence.remove(token)\n",
    "                \n",
    "    tag_sentence_list = []\n",
    "    \n",
    "    for token_sentence in token_sentence_list:\n",
    "        tag_list = nltk.tag.pos_tag(token_sentence)\n",
    "        tag_sentence_list.append(tag_list)\n",
    "        \n",
    "    #stem_sentence_list = []\n",
    "    \n",
    "    #for tag_sentence in tag_sentence_list:\n",
    "    #    print([rs.stem(word) for word, pos in tag_sentence])\n",
    "    #    stem_list = [rs.stem(word) for word, pos in tag_sentence]\n",
    "    #    stem_sentence_list.append(stem_list)\n",
    "    \n",
    "    #stem_tag_sentence_list = []\n",
    "    \n",
    "    #for stem_sentence in stem_sentence_list:\n",
    "    #    stem_tag_list = nltk.tag.pos_tag(stem_sentence)\n",
    "    #    stem_tag_sentence_list.append(stem_tag_list)\n",
    "    \n",
    "    for tag_sentence in tag_sentence_list:\n",
    "        \n",
    "        lemma_sentence = []\n",
    "        \n",
    "        for word, pos in tag_sentence:\n",
    "            if pos in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\"]:\n",
    "                pos='v'\n",
    "                lemma_sentence.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "            elif pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "                pos='n'\n",
    "                lemma_sentence.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "            elif pos in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "                pos='a'\n",
    "                lemma_sentence.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "            else:\n",
    "                lemma_sentence.append(lemmatizer.lemmatize(word))\n",
    "                \n",
    "        #print(\"Lemma 후: \", lemma_sentence)\n",
    "        return lemma_sentence\n",
    "\n",
    "# title열 토큰화\n",
    "combined['title_tokenized'] = combined['title'].apply(title_tokenize)\n",
    "combined = combined[(combined.title_tokenized.notnull())]\n",
    "combined.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('combined.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

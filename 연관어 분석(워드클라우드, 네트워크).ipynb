{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import network as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_list = list(stopwords.words('english'))\n",
    "print(english_stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_csv('combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>flair</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Korea introduces new point system for job seek...</td>\n",
       "      <td>36</td>\n",
       "      <td>9jw36t</td>\n",
       "      <td>https://www.youtube.com/watch?v=IrZHXBn8zLE</td>\n",
       "      <td>28</td>\n",
       "      <td>1538222833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>This is only makes getting a job seeking vis...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How to work legally in Korea on American citiz...</td>\n",
       "      <td>3</td>\n",
       "      <td>9jsxq6</td>\n",
       "      <td>https://www.reddit.com/r/korea/comments/9jsxq6...</td>\n",
       "      <td>22</td>\n",
       "      <td>1538186675</td>\n",
       "      <td>\\nI’m planning on going there for school so I ...</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>yeah hope ur conversational korean is good  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How one man transformed South Korea s sordid  ...</td>\n",
       "      <td>28</td>\n",
       "      <td>dry15c</td>\n",
       "      <td>https://www.reddit.com/r/korea/comments/dry15c...</td>\n",
       "      <td>6</td>\n",
       "      <td>1572954324</td>\n",
       "      <td>[https://www.cnbc.com/2019/11/05/how-yanolja-m...</td>\n",
       "      <td>Economy</td>\n",
       "      <td>You won t BELIEVE how this couple in lt your...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Does Korea have any words that describe landsc...</td>\n",
       "      <td>1</td>\n",
       "      <td>drww85</td>\n",
       "      <td>https://www.reddit.com/r/korea/comments/drww85...</td>\n",
       "      <td>9</td>\n",
       "      <td>1572946432</td>\n",
       "      <td>Hi I writing a novel about the natural history...</td>\n",
       "      <td>Nature</td>\n",
       "      <td>means stunning     Oh ok thanks  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Is it normal to give your parents allowances</td>\n",
       "      <td>9</td>\n",
       "      <td>drsn0w</td>\n",
       "      <td>https://www.reddit.com/r/korea/comments/drsn0w...</td>\n",
       "      <td>29</td>\n",
       "      <td>1572920637</td>\n",
       "      <td>I've overheard some foreign exchange students ...</td>\n",
       "      <td>Culture</td>\n",
       "      <td>I do that As I know most East asian and Sout...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  score  \\\n",
       "0           0  Korea introduces new point system for job seek...     36   \n",
       "1           1  How to work legally in Korea on American citiz...      3   \n",
       "2           2  How one man transformed South Korea s sordid  ...     28   \n",
       "3           3  Does Korea have any words that describe landsc...      1   \n",
       "4           4      Is it normal to give your parents allowances       9   \n",
       "\n",
       "       id                                                url  num_comments  \\\n",
       "0  9jw36t        https://www.youtube.com/watch?v=IrZHXBn8zLE            28   \n",
       "1  9jsxq6  https://www.reddit.com/r/korea/comments/9jsxq6...            22   \n",
       "2  dry15c  https://www.reddit.com/r/korea/comments/dry15c...             6   \n",
       "3  drww85  https://www.reddit.com/r/korea/comments/drww85...             9   \n",
       "4  drsn0w  https://www.reddit.com/r/korea/comments/drsn0w...            29   \n",
       "\n",
       "   created_utc                                           selftext  \\\n",
       "0   1538222833                                                NaN   \n",
       "1   1538186675  \\nI’m planning on going there for school so I ...   \n",
       "2   1572954324  [https://www.cnbc.com/2019/11/05/how-yanolja-m...   \n",
       "3   1572946432  Hi I writing a novel about the natural history...   \n",
       "4   1572920637  I've overheard some foreign exchange students ...   \n",
       "\n",
       "         flair                                           comments  sentiment  \n",
       "0  Immigration    This is only makes getting a job seeking vis...        1.0  \n",
       "1  Immigration    yeah hope ur conversational korean is good  ...        1.0  \n",
       "2      Economy    You won t BELIEVE how this couple in lt your...        1.0  \n",
       "3       Nature               means stunning     Oh ok thanks  ...        1.0  \n",
       "4      Culture    I do that As I know most East asian and Sout...        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사인 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title_tokenized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title_tokenized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f0cbe206cc06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 키워드 빈도 추출 - 어떤 단어가 주로 나오는지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlist4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title_tokenized'"
     ]
    }
   ],
   "source": [
    "# 키워드 빈도 추출 - 어떤 단어가 주로 나오는지\n",
    "list4 = list(itertools.chain(*combined['title_tokenized']))\n",
    "tokens_pos = nltk.pos_tag(list4)\n",
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사는 NN을 포함하고 있음을 알 수 있음\n",
    "NN_words = []\n",
    "for word, pos in tokens_pos:\n",
    "    if 'NN' in pos:\n",
    "        NN_words.append(word)\n",
    "print(NN_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    flatList=[]\n",
    "    for elem in l:\n",
    "        if type(elem) == list:\n",
    "            for e in elem:\n",
    "                flatList.append(e)\n",
    "        else:\n",
    "            flatList.append(elem)\n",
    "    return flatList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_words=flatten(NN_words)\n",
    "NN_words=[x for x in NN_words if len(x)>2]\n",
    "NN_words=[x for x in NN_words if not x.isdigit()]\n",
    "# 상위 50개 단어 frequency 뽑기 \n",
    "pd.Series(NN_words).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#직접 만든 불용어 사전의 용어 리스트에서 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_stopwords = ['http', 'amp', 'www', \"ec\", \"ve\",'eb','th','st'] # 직접 만든 불용어 사전\n",
    "\n",
    "unique_NN_words1 = set(NN_words)\n",
    "for word in unique_NN_words1:\n",
    "    if word in customized_stopwords:\n",
    "        while word in NN_words: NN_words.remove(word)\n",
    "\n",
    "print(NN_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여행과 관련된 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_word = list(itertools.chain(*datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_words=flatten(travel_word)\n",
    "NN_words=[x for x in NN_words if len(x)>2]\n",
    "NN_words=[x for x in NN_words if not x.isdigit()]\n",
    "# 상위 50개 단어 frequency 뽑기 \n",
    "pd.Series(NN_words).value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 명사 워드클라우드 그리기\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "noun_text = ''\n",
    "for word in NN_words:\n",
    "    noun_text = noun_text +' '+word\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=60, relative_scaling=.5).generate(noun_text) # generate() 는 하나의 string value를 입력 받음\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연관어 분석 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokenized_upper3 = []\n",
    "for post_word in combined['title_tokenized']:\n",
    "    for word in post_word:\n",
    "        if len(word) < 3:\n",
    "            post_word.remove(word)\n",
    "    title_tokenized_upper3.append(post_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['title_tokenized_upper3'] = title_tokenized_upper3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 태깅 다시 해주기\n",
    "\n",
    "title_tokenized_list = []\n",
    "for post_word in combined['title_tokenized_upper3']:\n",
    "    tag_list = nltk.tag.pos_tag(post_word)\n",
    "    title_tokenized_list.append(tag_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['title_tokenized_NN'] = title_tokenized_list\n",
    "combined['title_tokenized_NN']= combined['title_tokenized_NN'].apply(lambda x: [token for token, tag in x if tag in ('NN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined 값 리스트 형식 변환\n",
    "\n",
    "from ast import literal_eval\n",
    "combined['title_tokenized'] = combined['title_tokenized'].fillna('[]')\n",
    "combined['comments'] = combined['comments'].apply(literal_eval)\n",
    "combined['clean_title'] = combined['clean_title'].apply(literal_eval)\n",
    "combined['title_tokenized'] = combined['title_tokenized'].apply(literal_eval)\n",
    "combined['title_tokenized_NN'] = combined['title_tokenized_NN'].apply(literal_eval)\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_comments가 3 이하인 post 삭제\n",
    "for i in range(len(combined['title_tokenized'])):\n",
    "    for word in combined['title_tokenized'][i]:\n",
    "        if len(word) < 4:\n",
    "            print(word)\n",
    "            combined.iloc[i, -1].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 사전 추가 및 제거\n",
    "\n",
    "#직접 만든 불용어 사전의 용어 리스트에서 삭제\n",
    "\n",
    "customized_stopwords = ['http', 'amp', 'www', \"ec\", \"ve\",'eb','th','st','https','com','page','wiki','reddit'] # 직접 만든 불용어 사전\n",
    "\n",
    "list_stop=[]\n",
    "for post_word in combined['title_tokenized_NN']:\n",
    "    for word in post_word:\n",
    "        if word in customized_stopwords:\n",
    "            while word in post_word: post_word.remove(word)\n",
    "    list_stop.append(post_word)\n",
    "    \n",
    "combined['title_tokenized_NN_stop'] = list_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('./preprocessing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연관어 분석 - word2vec 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def model(keyword) :\n",
    "    model=Word2Vec(combined['title_tokenized_NN_stop'],sg=1, window=10, min_count=1)\n",
    "    model.init_sims(replace=True)\n",
    "    result = model.wv.most_similar(keyword, topn = 50)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['travel', 'seoul', 'tour', 'korea', 'trip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'travel', 'seoul', 'tour', 'korea', 'trip' 적용\n",
    "\n",
    "for i in words : \n",
    "    try :\n",
    "        print(i)\n",
    "        print(model(i))\n",
    "        print()\n",
    "    except : \n",
    "        print('해당 키워드는 출력할 수 없습니다.')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수설정\n",
    "\n",
    "travel = model(\"travel\")\n",
    "seoul = model(\"seoul\")\n",
    "tour = model(\"tour\")\n",
    "korea = model(\"korea\")\n",
    "trip = model(\"trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연관어 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ass_wordcloud(ass) : \n",
    "    words = []\n",
    "    for i, j in ass :\n",
    "        words.append(i)\n",
    "    \n",
    "    cloud_data =\" \".join(words)\n",
    "\n",
    "    from wordcloud import WordCloud\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white').generate(cloud_data)\n",
    "\n",
    "    plt.figure(figsize=(22,22)) #이미지 사이즈 지정\n",
    "    plt.imshow(wordcloud, interpolation='lanczos') #이미지의 부드럽기 정도\n",
    "    plt.axis('off') #x y 축 숫자 제거\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass_wordcloud('travel')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass_wordcloud('seoul')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass_wordcloud('tour')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass_wordcloud('korea')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass_wordcloud('trip')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 단어들 포함된 포스트 말뭉치 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_words(keyword) : \n",
    "    \n",
    "    words = []\n",
    "    for i, j in ass :\n",
    "        words.append(i)\n",
    "        \n",
    "    word_post_indexes = []\n",
    "    for i in range(len(combined)) : \n",
    "        if (keyword in combined['title_tokenized_NN_stop'][i]) ==True : \n",
    "            for j in range(len(combined.iloc[i, -1])) : \n",
    "                if combined['title_tokenized_NN_stop'][i][j] in words : \n",
    "                    word_post_indexes.append(i)\n",
    "                    break\n",
    "                    \n",
    "    word_post_datas = []\n",
    "    for i in combined.iloc[word_post_indexes, -1] : \n",
    "        word_post_datas.append(i)\n",
    "    \n",
    "    return word_post_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크 그리기 - 특정 단어 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(keyword) : \n",
    "    \n",
    "    post_datas = post_words(keyword)\n",
    "    \n",
    "    from apyori import apriori\n",
    "\n",
    "    result=(list(apriori(post_datas, min_support=0.2)))\n",
    "    df=pd.DataFrame(result)\n",
    "\n",
    "    df['length']=df['items'].apply(lambda x: len(x))\n",
    "    df=df[(df['length']==2)&(df['support']>=0.05)].sort_values(by='support', ascending=False)\n",
    "    \n",
    "    G=nx.Graph()\n",
    "    G.add_edges_from(df['items'])\n",
    "    pr=nx.pagerank(G)\n",
    "    nsize=np.array([v for v in pr.values()])\n",
    "    nsize=2000*(nsize-min(nsize))/(max(nsize)-min(nsize))\n",
    "    pos=nx.spring_layout(G)\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(G, font_size=10, pos=pos, node_color=list(pr.values()), node_size=nsize,\n",
    "                     alpha=0.5, edge_color='.7', cmap=plt.cm.YlGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netword('travel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netword('seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netword('tour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netword('korea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netword('trip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
